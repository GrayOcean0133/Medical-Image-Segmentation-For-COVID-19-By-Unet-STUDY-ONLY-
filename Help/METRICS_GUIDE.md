# è®­ç»ƒæŒ‡æ ‡ç›‘æŽ§ç³»ç»Ÿä½¿ç”¨æŒ‡å—

## æ¦‚è¿°

æ–°çš„è®­ç»ƒæŒ‡æ ‡ç›‘æŽ§ç³»ç»Ÿå¯ä»¥è¯¦ç»†è®°å½•è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ‰€æœ‰å…³é”®æŒ‡æ ‡ï¼ŒåŒ…æ‹¬Losså€¼ã€å­¦ä¹ çŽ‡ç­‰ï¼Œå¹¶æä¾›å¯è§†åŒ–åˆ†æžå·¥å…·ï¼Œå¸®åŠ©ä½ æ·±å…¥ç†è§£æ¨¡åž‹è®­ç»ƒè¿‡ç¨‹ã€‚

## åŠŸèƒ½ç‰¹æ€§

### 1. è‡ªåŠ¨è®°å½•è®­ç»ƒæŒ‡æ ‡
- **Epochçº§åˆ«**ï¼šæ¯ä¸ªepochçš„å¹³å‡lossã€æœ€å°/æœ€å¤§batch lossã€å­¦ä¹ çŽ‡ã€è®­ç»ƒæ—¶é—´ç­‰
- **Batchçº§åˆ«ï¼ˆå¯é€‰ï¼‰**ï¼šæ¯ä¸ªbatchçš„è¯¦ç»†losså’Œå­¦ä¹ çŽ‡è®°å½•

### 2. å®žæ—¶è¿›åº¦æ˜¾ç¤º
- è®­ç»ƒè¿›åº¦ç™¾åˆ†æ¯”
- å½“å‰batchçš„losså€¼
- é¢„è®¡å‰©ä½™æ—¶é—´ï¼ˆETAï¼‰

### 3. CSVæ–‡ä»¶å¯¼å‡º
- ç»“æž„åŒ–çš„CSVæ ¼å¼ï¼Œæ–¹ä¾¿åŽç»­åˆ†æž
- æ—¶é—´æˆ³å‘½åï¼Œä¸ä¼šè¦†ç›–åŽ†å²æ•°æ®

### 4. å¯è§†åŒ–åˆ†æž
- Lossæ›²çº¿å›¾
- å­¦ä¹ çŽ‡å˜åŒ–å›¾
- è®­ç»ƒæ—¶é—´ç»Ÿè®¡
- è¶‹åŠ¿åˆ†æž

---

## å¿«é€Ÿå¼€å§‹

### åŸºç¡€è®­ç»ƒï¼ˆåªè®°å½•Epochçº§åˆ«ï¼‰

```bash
python src/train_ddp.py
```

è¿™ä¼šï¼š
- âœ… æ¯10ä¸ªbatchè¾“å‡ºä¸€æ¬¡è¿›åº¦ï¼ˆé»˜è®¤ï¼‰
- âœ… è‡ªåŠ¨è®°å½•æ¯ä¸ªepochçš„ç»Ÿè®¡ä¿¡æ¯
- âœ… ç”Ÿæˆ`metrics/epoch_metrics_TIMESTAMP.csv`æ–‡ä»¶

### è¯¦ç»†è®­ç»ƒï¼ˆè®°å½•Batchçº§åˆ«ï¼‰

```bash
python src/train_ddp.py --log_batch_metrics
```

è¿™ä¼šï¼š
- âœ… è®°å½•æ¯ä¸ªepochçš„ç»Ÿè®¡ä¿¡æ¯
- âœ… **é¢å¤–**è®°å½•æ¯ä¸ªbatchçš„è¯¦ç»†losså€¼
- âœ… ç”Ÿæˆä¸¤ä¸ªæ–‡ä»¶ï¼š
  - `metrics/epoch_metrics_TIMESTAMP.csv`
  - `metrics/batch_metrics_TIMESTAMP.csv`

âš ï¸ **æ³¨æ„**ï¼šBatchçº§åˆ«è®°å½•ä¼šç”Ÿæˆå¤§é‡æ•°æ®ï¼ˆ200 epochs Ã— 171 batches â‰ˆ 34,000è¡Œï¼‰

### è°ƒæ•´æ—¥å¿—è¾“å‡ºé¢‘çŽ‡

```bash
# æ¯5ä¸ªbatchè¾“å‡ºä¸€æ¬¡ï¼ˆæ›´é¢‘ç¹ï¼‰
python src/train_ddp.py --log_interval 5

# æ¯50ä¸ªbatchè¾“å‡ºä¸€æ¬¡ï¼ˆæ›´å°‘ï¼‰
python src/train_ddp.py --log_interval 50
```

---

## è®­ç»ƒæ—¶çœ‹åˆ°ä»€ä¹ˆ

### 1. å¯åŠ¨ä¿¡æ¯
```
å·¥ä½œç›®å½•: /workspace/GrayOcean/code/Medic_Project
æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•: .../checkpoints
ç»“æžœä¿å­˜ç›®å½•: .../results
æ—¥å¿—ä¿å­˜ç›®å½•: .../log
è®­ç»ƒæŒ‡æ ‡ä¿å­˜ç›®å½•: .../metrics
================================================================================
è®­ç»ƒé…ç½®:
  Epochs: 200
  Batch Size (per GPU): 4
  Learning Rate: 0.001
  ...
æ—¥å¿—é…ç½®:
  æ—¥å¿—è¾“å‡ºé—´éš”: æ¯10ä¸ªbatch
  è®°å½•Batchçº§åˆ«æŒ‡æ ‡: å¦
================================================================================
EpochæŒ‡æ ‡è®°å½•æ–‡ä»¶: .../metrics/epoch_metrics_20251106_103015.csv
================================================================================
```

### 2. è®­ç»ƒè¿‡ç¨‹
```
Epoch [1/200] - è¿›åº¦ 0.0% (0/171) - Batch Loss: 0.523415 - LR: 0.00100000
Epoch [1/200] - è¿›åº¦ 5.8% (10/171) - Batch Loss: 0.478203 - LR: 0.00100000
...
Epoch [1/200] - è¿›åº¦ 58.5% (100/171) - Batch Loss: 0.312456 - LR: 0.00100000
...
================================================================================
Epoch [1/200] å®Œæˆ
å¹³å‡æŸå¤±: 0.425316
æœ€å°BatchæŸå¤±: 0.285432
æœ€å¤§BatchæŸå¤±: 0.623571
å­¦ä¹ çŽ‡: 0.00100000
æœ¬epochç”¨æ—¶: 14.52ç§’
é¢„è®¡å‰©ä½™æ—¶é—´: 0.80å°æ—¶
================================================================================
```

### 3. æ¨¡åž‹ä¿å­˜
```
âœ“ å·²ä¿å­˜æ£€æŸ¥ç‚¹: model_epoch_10.pth
âœ“ æ–°çš„æœ€ä½³æ¨¡åž‹! Loss: 0.325416 (æå‡: 0.012345)
âœ“ å·²ä¿å­˜æœ€ä½³æ¨¡åž‹: best_model.pth
```

---

## ç”Ÿæˆçš„æ•°æ®æ–‡ä»¶

### ç›®å½•ç»“æž„
```
Medic_Project/
â”œâ”€â”€ metrics/                                    # è®­ç»ƒæŒ‡æ ‡ç›®å½•
â”‚   â”œâ”€â”€ epoch_metrics_20251106_103015.csv      # Epochçº§åˆ«æŒ‡æ ‡
â”‚   â””â”€â”€ batch_metrics_20251106_103015.csv      # Batchçº§åˆ«æŒ‡æ ‡ï¼ˆå¯é€‰ï¼‰
â”œâ”€â”€ log/
â”‚   â””â”€â”€ training_20251106_103015.log           # è¯¦ç»†æ—¥å¿—
â””â”€â”€ analysis/                                   # åˆ†æžç»“æžœï¼ˆè¿è¡Œåˆ†æžè„šæœ¬åŽï¼‰
    â”œâ”€â”€ training_analysis_epoch_metrics_*.png
    â”œâ”€â”€ training_stats_epoch_metrics_*.png
    â””â”€â”€ training_report_epoch_metrics_*.txt
```

### Epoch Metrics CSV æ ¼å¼

| åˆ—å | è¯´æ˜Ž | ç¤ºä¾‹ |
|------|------|------|
| Epoch | Epochç¼–å· | 1, 2, 3, ... |
| Avg_Loss | è¯¥epochçš„å¹³å‡loss | 0.425316 |
| Min_Batch_Loss | è¯¥epochä¸­æœ€å°çš„batch loss | 0.285432 |
| Max_Batch_Loss | è¯¥epochä¸­æœ€å¤§çš„batch loss | 0.623571 |
| Learning_Rate | å½“å‰å­¦ä¹ çŽ‡ | 0.00100000 |
| Epoch_Time_Seconds | è¯¥epochè®­ç»ƒæ—¶é—´ï¼ˆç§’ï¼‰ | 14.52 |
| Best_Loss_So_Far | ç›®å‰ä¸ºæ­¢çš„æœ€ä½³loss | 0.425316 |
| Is_Best_Model | æ˜¯å¦ä¸ºæœ€ä½³æ¨¡åž‹ | True/False |
| Timestamp | æ—¶é—´æˆ³ | 2025-11-06 10:30:15 |

### Batch Metrics CSV æ ¼å¼

| åˆ—å | è¯´æ˜Ž | ç¤ºä¾‹ |
|------|------|------|
| Epoch | Epochç¼–å· | 1 |
| Batch | Batchç¼–å· | 0, 1, 2, ... |
| Loss | è¯¥batchçš„losså€¼ | 0.523415 |
| Learning_Rate | å½“å‰å­¦ä¹ çŽ‡ | 0.00100000 |
| Timestamp | æ—¶é—´æˆ³ | 2025-11-06 10:30:15 |

---

## è®­ç»ƒåŽåˆ†æž

### 1. è¿è¡Œè‡ªåŠ¨åˆ†æžè„šæœ¬

```bash
# åˆ†æžæœ€æ–°çš„epochçº§åˆ«æ•°æ®
python analyze_training.py

# åˆ†æžç‰¹å®šçš„metricsæ–‡ä»¶
python analyze_training.py --metrics_file metrics/epoch_metrics_20251106_103015.csv

# åˆ†æžbatchçº§åˆ«æ•°æ®
python analyze_training.py --batch_metrics
```

### 2. ç”Ÿæˆçš„åˆ†æžæ–‡ä»¶

#### a) Lossæ›²çº¿å›¾ (training_analysis_*.png)
åŒ…å«4ä¸ªå­å›¾ï¼š
1. **Training Loss Curve**ï¼šå¹³å‡losså’Œæœ€ä½³losså¯¹æ¯”
2. **Loss Range**ï¼šæ¯ä¸ªepochçš„lossæ³¢åŠ¨èŒƒå›´
3. **Learning Rate Schedule**ï¼šå­¦ä¹ çŽ‡å˜åŒ–æ›²çº¿
4. **Time per Epoch**ï¼šæ¯ä¸ªepochçš„è®­ç»ƒæ—¶é—´

#### b) ç»Ÿè®¡åˆ†æžå›¾ (training_stats_*.png)
åŒ…å«2ä¸ªå­å›¾ï¼š
1. **Loss Improvement per Epoch**ï¼šæ¯ä¸ªepochçš„lossæ”¹å–„å¹…åº¦
2. **Batch Loss Stability**ï¼šbatché—´lossçš„ç¨³å®šæ€§åˆ†æž

#### c) æ–‡æœ¬æŠ¥å‘Š (training_report_*.txt)
```
================================================================================
è®­ç»ƒåˆ†æžæŠ¥å‘Š
================================================================================
æ•°æ®æ–‡ä»¶: metrics/epoch_metrics_20251106_103015.csv
ç”Ÿæˆæ—¶é—´: 2025-11-06 12:30:45

è®­ç»ƒæ¦‚è§ˆ:
  æ€»Epochæ•°: 200
  æ€»è®­ç»ƒæ—¶é—´: 0.80å°æ—¶
  å¹³å‡æ¯Epochæ—¶é—´: 14.40ç§’

Lossç»Ÿè®¡:
  åˆå§‹Loss: 0.523415
  æœ€ç»ˆLoss: 0.067721
  æœ€ä½³Loss: 0.065432 (Epoch 195)
  Lossæ”¹å–„: 0.455694 (87.06%)

å­¦ä¹ çŽ‡:
  åˆå§‹å­¦ä¹ çŽ‡: 0.00100000
  æœ€ç»ˆå­¦ä¹ çŽ‡: 0.00100000

Top 5æœ€ä½³Epochs:
  Epoch 195: Loss=0.065432, LR=0.00100000
  Epoch 198: Loss=0.067215, LR=0.00100000
  Epoch 192: Loss=0.067721, LR=0.00100000
  ...
```

---

## æ‰‹åŠ¨åˆ†æžæ•°æ®

### ä½¿ç”¨Python (pandas)

```python
import pandas as pd
import matplotlib.pyplot as plt

# è¯»å–æ•°æ®
df = pd.read_csv('metrics/epoch_metrics_20251106_103015.csv')

# æŸ¥çœ‹åŸºæœ¬ç»Ÿè®¡
print(df.describe())

# ç»˜åˆ¶lossæ›²çº¿
plt.figure(figsize=(10, 6))
plt.plot(df['Epoch'], df['Avg_Loss'])
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.savefig('my_loss_curve.png')

# æ‰¾å‡ºlossæœ€ä½Žçš„5ä¸ªepochs
best_epochs = df.nsmallest(5, 'Avg_Loss')
print(best_epochs)
```

### ä½¿ç”¨Excel

1. æ‰“å¼€CSVæ–‡ä»¶ï¼š`metrics/epoch_metrics_*.csv`
2. æ’å…¥å›¾è¡¨ â†’ æŠ˜çº¿å›¾
3. é€‰æ‹©æ•°æ®èŒƒå›´ï¼š
   - Xè½´ï¼šEpochåˆ—
   - Yè½´ï¼šAvg_Lossåˆ—
4. è‡ªå®šä¹‰å›¾è¡¨æ ·å¼

---

## å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆçœ‹ä¸åˆ°å®žæ—¶çš„batch lossï¼Ÿ
**A**: é»˜è®¤æ¯10ä¸ªbatchè¾“å‡ºä¸€æ¬¡ã€‚å¯ä»¥é€šè¿‡`--log_interval 1`æ¯ä¸ªbatchéƒ½è¾“å‡ºï¼Œä½†ä¼šäº§ç”Ÿå¤§é‡æ—¥å¿—ã€‚

### Q2: batch_metricsæ–‡ä»¶å¤ªå¤§æ€Žä¹ˆåŠžï¼Ÿ
**A**:
- å¦‚æžœåªéœ€è¦äº†è§£è®­ç»ƒè¶‹åŠ¿ï¼Œepochçº§åˆ«çš„æ•°æ®å°±è¶³å¤Ÿäº†
- batchçº§åˆ«æ•°æ®ä¸»è¦ç”¨äºŽï¼š
  - è°ƒè¯•è®­ç»ƒä¸ç¨³å®šé—®é¢˜
  - åˆ†æžè¿‡æ‹ŸåˆçŽ°è±¡
  - ç ”ç©¶batch sizeå½±å“

### Q3: å¦‚ä½•å¯¹æ¯”ä¸¤æ¬¡è®­ç»ƒçš„ç»“æžœï¼Ÿ
**A**:
```python
import pandas as pd
import matplotlib.pyplot as plt

# è¯»å–ä¸¤æ¬¡è®­ç»ƒçš„æ•°æ®
df1 = pd.read_csv('metrics/epoch_metrics_run1.csv')
df2 = pd.read_csv('metrics/epoch_metrics_run2.csv')

# ç»˜åˆ¶å¯¹æ¯”å›¾
plt.figure(figsize=(10, 6))
plt.plot(df1['Epoch'], df1['Avg_Loss'], label='Run 1')
plt.plot(df2['Epoch'], df2['Avg_Loss'], label='Run 2')
plt.legend()
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Comparison')
plt.savefig('comparison.png')
```

### Q4: metricsæ–‡ä»¶ä¼šè‡ªåŠ¨è¦†ç›–å—ï¼Ÿ
**A**: ä¸ä¼šã€‚æ¯æ¬¡è®­ç»ƒéƒ½ä¼šåˆ›å»ºå¸¦æ—¶é—´æˆ³çš„æ–°æ–‡ä»¶ï¼Œæ‰€ä»¥å¯ä»¥ä¿ç•™åŽ†å²è®­ç»ƒè®°å½•ã€‚

### Q5: å¦‚ä½•åˆ é™¤æ—§çš„metricsæ–‡ä»¶ï¼Ÿ
**A**:
```bash
# åªä¿ç•™æœ€è¿‘7å¤©çš„metrics
find metrics/ -name "*.csv" -mtime +7 -delete

# åªä¿ç•™æœ€æ–°çš„3ä¸ªmetricsæ–‡ä»¶
cd metrics
ls -t epoch_metrics_*.csv | tail -n +4 | xargs rm -f
```

---

## é«˜çº§ä½¿ç”¨

### 1. æ·»åŠ è‡ªå®šä¹‰æŒ‡æ ‡

ç¼–è¾‘`train_ddp.py`ä¸­çš„MetricsLoggerç±»ï¼š

```python
# åœ¨MetricsLogger._init_epoch_logger()ä¸­æ·»åŠ æ–°åˆ—
headers = [
    'Epoch',
    'Avg_Loss',
    'Val_Loss',  # æ–°å¢žï¼šéªŒè¯é›†loss
    'Train_Acc',  # æ–°å¢žï¼šè®­ç»ƒå‡†ç¡®çŽ‡
    ...
]

# åœ¨log_epoch()ä¸­ä¼ å…¥æ–°æ•°æ®
def log_epoch(self, epoch, avg_loss, val_loss, train_acc, ...):
    row = [epoch, avg_loss, val_loss, train_acc, ...]
    ...
```

### 2. å®žæ—¶ç›‘æŽ§è„šæœ¬

```python
# watch_training.py
import pandas as pd
import time
from pathlib import Path

def watch_latest_metrics():
    metrics_dir = Path("metrics")
    files = list(metrics_dir.glob("epoch_metrics_*.csv"))
    latest = max(files, key=lambda p: p.stat().st_mtime)

    print(f"ç›‘æŽ§æ–‡ä»¶: {latest}")
    last_size = 0

    while True:
        try:
            current_size = latest.stat().st_size
            if current_size > last_size:
                df = pd.read_csv(latest)
                latest_row = df.iloc[-1]
                print(f"\nEpoch {latest_row['Epoch']}: "
                      f"Loss={latest_row['Avg_Loss']:.6f}, "
                      f"LR={latest_row['Learning_Rate']:.8f}")
                last_size = current_size
            time.sleep(5)
        except KeyboardInterrupt:
            break

if __name__ == "__main__":
    watch_latest_metrics()
```

---

## æ€»ç»“

### åŸºç¡€ç”¨æˆ·ï¼ˆåªå…³å¿ƒæœ€ç»ˆç»“æžœï¼‰
```bash
python src/train_ddp.py
# è®­ç»ƒå®ŒæˆåŽ
python analyze_training.py
```

### å­¦ä¹ ç”¨æˆ·ï¼ˆæƒ³äº†è§£è®­ç»ƒç»†èŠ‚ï¼‰
```bash
# è®°å½•è¯¦ç»†çš„batchæ•°æ®
python src/train_ddp.py --log_batch_metrics --log_interval 5

# åˆ†æžepochæ•°æ®
python analyze_training.py

# åˆ†æžbatchæ•°æ®
python analyze_training.py --batch_metrics
```

### ç ”ç©¶ç”¨æˆ·ï¼ˆæ·±åº¦åˆ†æžï¼‰
```bash
# å¯¼å‡ºæ•°æ®åŽä½¿ç”¨Python/Excelè¿›è¡Œè‡ªå®šä¹‰åˆ†æž
import pandas as pd
df = pd.read_csv('metrics/epoch_metrics_*.csv')
# è‡ªå®šä¹‰åˆ†æž...
```

## [-> è¿”å›žREADME](../README.md)


**Happy Training! ðŸš€**

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶ï¼š`log/training_*.log`


